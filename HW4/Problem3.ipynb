{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f503f4-3e88-42d5-b8e3-acc7eb8da84b",
   "metadata": {},
   "source": [
    "# Homework 4 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00902264-a533-4fd9-bb12-0507b76e2c1c",
   "metadata": {},
   "source": [
    "#### Kush Lalwani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446dea17-5606-44ce-b021-909b2e8994bf",
   "metadata": {},
   "source": [
    "### Load Data and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1696c61b-3917-4ad1-b73c-e987ef46d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load data in LibSVM sparse data format\n",
    "X_train, y_train = load_svmlight_file(\"a9aTrain.txt\")\n",
    "X_test, y_test = load_svmlight_file(\"a9aTest.t\")\n",
    "\n",
    "#Convert the -1/1 labels to a 0/1 label since that is the expected input for the classifiers I am using\n",
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95955274-db84-4c51-bd3c-15b4063f3715",
   "metadata": {},
   "source": [
    "### Chosen Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116bb268-fa21-483c-8364-3ea476cc30b0",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c456ac-92ee-4268-960d-f919623b7f2f",
   "metadata": {},
   "source": [
    "Random Forest is an Ensemble learning method that uses multiple decision trees to classify data. Each decision tree is trained on a bootstrapped sample of the original dataset, so this is considered a Bagging algorithm. Each of these smaller decision tree also only uses a subset of the features. The model will then take a vote based on the smaller decision trees that were created, allowing the model to classify a test point. This algorithm avoids the regular decision tree's tendency to overfit the data. Random Forest tends to reduce variance and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbede6-4594-4e4c-8687-eff72a8ae79a",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf7ff0f-b0aa-4732-8ada-da8a8a988cee",
   "metadata": {},
   "source": [
    "XGBoost is also an ensemble learning method that utilizes weak learners, often decision trees. This is a Boosting algorithm, since it starts with weak learners and slowly builds off of them to create a strong learner. This algorithm also includes a regularization term to prevent against overfitting of the data. Boosting is iterative, if previous data were misclassified in the previous weak learner, the next model will try to put more importance on the previously misclassified points. XGBoost tends to reduce bias and prevent against underfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b61fa-c890-449d-9550-ae1642571cfe",
   "metadata": {},
   "source": [
    "### Description of Training Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db967bdc-56fe-48eb-8b6f-8c8c39b15611",
   "metadata": {},
   "source": [
    "First I will set a seed for the training, so that all of the code is exactly reproducable. Then I will fit the basic models with all of the default parameters to the training data, to get a baseline sense of the accuracy that can be achieved with the respective models. Once I have the baseline accuracy, I will run 10-fold cross validation to find the best possible parameter for the models. In order to do this, I will use the gridsearch function alongside the cross validation to test every single combination of paramters. I will first initialize a list that will be used for possible value of a parameter. This will save a lot of space as compared to manually writing validation code to test every single parameter combination which could involve many nested loops to test parameters. I will do this to train the model for both the random forest model as well as the decision trees. \n",
    "\n",
    "**Edit**: After trying to run the code using cross validation with the grid search, it took too long to run. So I decided on an alternative approach, which was to use a random search to do my cross validation. Random searching will randomly sample a subset of possible parameter values rather than computing every single possible combination of parameters. This random searching will ensure that not too many models end up being trained, which will take too long to run. I also decided to opt for 3-fold cross validation instead of 10, which also significantly reduced the amount of time the code takes to run. Otherwise, my approach was the exact same except for the fact that I used random search instead of gridsearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357ffd75-6ffe-4b51-9fa9-621951e65496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a 80/20 training validation split for the baseline classifiers\n",
    "np.random.seed(448)\n",
    "\n",
    "seed = 448\n",
    "test_size = 0.2\n",
    "X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(X_train, y_train, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6b1b5-098c-4de3-8266-2d9279377a70",
   "metadata": {},
   "source": [
    "#### Random Forest Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762566a8-224c-4e5f-b2a4-651e9ed559cd",
   "metadata": {},
   "source": [
    "The Random Forest Classifier has the following parameters that are to be trained: \n",
    "1. **n_estimators**: This parameter tunes the number of tree models that are trained, the default value for this parameter is `100`. This means that there are 100 decision trees trained in this algorithm by default.\n",
    "2. **bootstrap**: This parameter is used to decide whether the algorithm uses bootstrapped samples of the whole dataset. Bootstrap samples are a random resampling of the dataset with replacement. By default the parameter is set to `True`; if it is set to `False`, it will use the entire dataset to train the model.\n",
    "3. **max_depth**: This parameter tunes the maximum depth of each decision tree in the model. This by default, is set to `None`. If the model is trained with the default parameter, the tree will continue to expand until each of the nodes are pure; meaning that each leaf node will all contain the same class. \n",
    "4. **min_impurity_decrease**: This parameter tunes how the nodes determine which feature to split on. The model will split on a proposed feature if the impurity score decrease of the model is greater than or equal to the set threshold. By default, the parameter is set to `0.0`. \n",
    "5. **min_samples_leaf**: This parameter tunes the minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. By default, the parameter is set to `1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fafe081f-410e-4261-ad41-a23019674508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.85%\n"
     ]
    }
   ],
   "source": [
    "baseline_rf = RandomForestClassifier(random_state=448) #set the random state because the random seed is not enough to ensure the reproducability\n",
    "baseline_rf.fit(X_train_base,y_train_base)\n",
    "\n",
    "y_pred_base_rf = baseline_rf.predict(X_val_base)\n",
    "predictions = [round(value) for value in y_pred_base_rf]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_val_base, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e72d8f-525e-4ea1-b1c1-320b4e59c18c",
   "metadata": {},
   "source": [
    "Now that we have the baseline accuracy of 82.85%, we can run the cross validation using the grid. We will then see which combination of parameters will have the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6f15a4-4c0b-4e57-ba5f-16aa3f94c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters: {'n_estimators': 200, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0, 'max_depth': 20, 'bootstrap': False}\n",
      "Best Cross-Validation Accuracy: 0.845305826941385\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'bootstrap': [True, False],  # Whether to use bootstrapping\n",
    "    'max_depth': [10, 20, 30, None],  # Maximum depth of trees\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.05],  # Threshold for a node to split\n",
    "    'min_samples_leaf': [1, 2, 5]  # Minimum samples per leaf node\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=448)\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf,  # Random Forest model\n",
    "    param_distributions=param_grid_rf,  # Parameter grid\n",
    "    n_iter=10,  # Number of random samples to try\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='accuracy',  \n",
    "    n_jobs=-1, \n",
    "    verbose=2,  \n",
    "    random_state=448  \n",
    ")\n",
    "\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", random_search_rf.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", random_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ebdac-89be-4a1f-98bf-1d709577a7de",
   "metadata": {},
   "source": [
    "As we can see, the best parameters with our cross validation were found to be:\n",
    "- n_estimators: 200\n",
    "- bootstrap: False\n",
    "- max_depth: 20\n",
    "- min_impurity_decrease: 0.0\n",
    "- min_samples_leaf: 5\n",
    "\n",
    "And we found the accuracy with this set of parameters to be: 84.53%\n",
    "So then the Cross Validation error will be: 15.47%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb971d-e98c-4405-90c4-e582498ad91e",
   "metadata": {},
   "source": [
    "### Create the table of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72026569-af73-4a06-9baf-d584a8c4f3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_n_estimators  param_max_depth  param_min_samples_leaf  \\\n",
      "3                 200               20                       5   \n",
      "7                 100               10                       5   \n",
      "9                 100               30                       1   \n",
      "2                  50               30                       5   \n",
      "1                 100               30                       1   \n",
      "0                 100               20                       5   \n",
      "5                  50               30                       1   \n",
      "4                  50               20                       1   \n",
      "6                  50               20                       1   \n",
      "8                 100               10                       1   \n",
      "\n",
      "   param_min_impurity_decrease  param_bootstrap  cv_accuracy  error_rate  \n",
      "3                         0.00            False     0.845306    0.154694  \n",
      "7                         0.00            False     0.840177    0.159823  \n",
      "9                         0.00             True     0.834803    0.165197  \n",
      "2                         0.01             True     0.760572    0.239428  \n",
      "1                         0.01             True     0.759190    0.240810  \n",
      "0                         0.05             True     0.759190    0.240810  \n",
      "5                         0.05            False     0.759190    0.240810  \n",
      "4                         0.01            False     0.759190    0.240810  \n",
      "6                         0.05             True     0.759190    0.240810  \n",
      "8                         0.01            False     0.759190    0.240810  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(random_search_rf.cv_results_)[\n",
    "    ['param_n_estimators', 'param_max_depth', 'param_min_samples_leaf', \n",
    "     'param_min_impurity_decrease', 'param_bootstrap', 'mean_test_score']\n",
    "]\n",
    "\n",
    "# Compute error rate\n",
    "results_df['error_rate'] = 1 - results_df['mean_test_score']\n",
    "\n",
    "# Sort by best accuracy\n",
    "results_df = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# Rename columns for clarity\n",
    "results_df.rename(columns={'mean_test_score': 'cv_accuracy'}, inplace=True)\n",
    "\n",
    "# Display table\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, save the table to a CSV for your report\n",
    "results_df.to_csv(\"rf_hyperparameter_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee33916-a905-41b3-9e36-fad06c7a7e36",
   "metadata": {},
   "source": [
    "#### XGBoost Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8bae4f-af41-412c-9657-e2209b2e9bf8",
   "metadata": {},
   "source": [
    "The XGBoost classifier has the following parameters that are to be trained:\n",
    "1. **n_estimators**: This parameter is the amount of estimators that the model will use. So this will be the amount of iterations of boosting that the model will have. By default the parameter will take value `1`. \n",
    "2. **max_depth**: This parameter tunes the depth of the decision trees that are used as the weak learners for this model. By default this parameter is set to `6`.\n",
    "3. **reg_lambda**: This parameter tunes the lambda value of the L2 regularization parameter. By default, it is set to `1.0`.\n",
    "4. **learning_rate**: This is the learning rate of the algorithm which is similar to the eta parameter for gradient descent. By default, the parameter is set to `0.3`.\n",
    "5. **missing**: This parameter is the value in the data which needs to be present as a missing value. By default it is set to `numpy.nan`. It take a float.\n",
    "6. **objective**: This parameter determines the learning task of the algorithm. You can also change the objective function that is being optimized. You can also set a custom objective function that will be optimized. It takes in a scikit learn objective function that should be optimized. By default it is set to a binary logistic classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6c679d-07e0-43ef-9b20-16d7c3010c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.43%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(448)\n",
    "\n",
    "xgboost_base = XGBClassifier()\n",
    "xgboost_base.fit(X_train_base,y_train_base)\n",
    "\n",
    "y_pred_base_xgb = xgboost_base.predict(X_val_base)\n",
    "predictions_xgb = [round(value) for value in y_pred_base_xgb]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_val_base, predictions_xgb)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354bbd0d-71f1-4afd-9b7a-9fb34e546682",
   "metadata": {},
   "source": [
    "As we can see the baseline accuracy is 84.43%, now we can optimize the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ea1c30-fb4f-4c9d-b09b-eec717ca5e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters: {'reg_lambda': 1, 'objective': 'binary:logistic', 'n_estimators': 200, 'missing': nan, 'max_depth': None, 'learning_rate': 0.05}\n",
      "Best Cross-Validation Accuracy: 0.8495747649631169\n"
     ]
    }
   ],
   "source": [
    "param_dist_xgb = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [3, 6, 10, None],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"reg_lambda\": [0, 0.1, 1], \n",
    "    \"missing\": [np.nan],  # We dont actually want to tune this parameter because it will just treat missing values as NaN\n",
    "    \"objective\": [\"binary:logistic\"] # We also shouldn't change this because we have a classification problem that we are dealing with\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(random_state=448)\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=10,  # Try 10 random parameter combinations\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=2,  # Print progress\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", xgb_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", xgb_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7537b5-3213-4cfd-a4a4-1e069dcf1b9e",
   "metadata": {},
   "source": [
    "As we can see, the best parameters with our cross validation were found to be:\n",
    "- n_estimators: 200\n",
    "- max_depth: None\n",
    "- lambda: 1\n",
    "- learning_rate: 0.05\n",
    "- missing: np.nan\n",
    "- objective: binary:logistic\n",
    "\n",
    "\n",
    "And we found the accuracy with this set of parameters to be: 84.96%\n",
    "So then the Cross-Validation error will be 15.04%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d90b31-cf39-4b12-97ba-2622cbc8bc5e",
   "metadata": {},
   "source": [
    "### Create the table of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee5d24f-7880-4d20-8453-f1f7b432524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_n_estimators  param_max_depth  param_min_samples_leaf  \\\n",
      "3                 200               20                       5   \n",
      "7                 100               10                       5   \n",
      "9                 100               30                       1   \n",
      "2                  50               30                       5   \n",
      "1                 100               30                       1   \n",
      "0                 100               20                       5   \n",
      "5                  50               30                       1   \n",
      "4                  50               20                       1   \n",
      "6                  50               20                       1   \n",
      "8                 100               10                       1   \n",
      "\n",
      "   param_min_impurity_decrease  mean_test_score  error_rate  \n",
      "3                         0.00         0.845306    0.154694  \n",
      "7                         0.00         0.840177    0.159823  \n",
      "9                         0.00         0.834803    0.165197  \n",
      "2                         0.01         0.760572    0.239428  \n",
      "1                         0.01         0.759190    0.240810  \n",
      "0                         0.05         0.759190    0.240810  \n",
      "5                         0.05         0.759190    0.240810  \n",
      "4                         0.01         0.759190    0.240810  \n",
      "6                         0.05         0.759190    0.240810  \n",
      "8                         0.01         0.759190    0.240810  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(random_search_rf.cv_results_)[\n",
    "    ['param_n_estimators', 'param_max_depth', 'param_min_samples_leaf', \n",
    "     'param_min_impurity_decrease', 'mean_test_score']\n",
    "]\n",
    "\n",
    "# Convert accuracy to error rate\n",
    "results_df['error_rate'] = 1 - results_df['mean_test_score']\n",
    "\n",
    "# Sort by best accuracy\n",
    "results_df = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "# Display the table\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0993325-e22b-4bb0-96eb-bf411d8c64d5",
   "metadata": {},
   "source": [
    "## Run the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "118bdd14-10a0-4395-b0b4-7e2f081ba087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Accuracy: 0.8485\n"
     ]
    }
   ],
   "source": [
    "# Get the best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Test Accuracy: {accuracy_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef942f-64ab-4d2a-b9b9-d92ca1df5587",
   "metadata": {},
   "source": [
    "For the random forest model, the accuracy on the test set is 84.85% which is similar to the accuracy that we had for the training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d79ea14-8f18-4c88-86c5-dfc2c199462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test Accuracy: 0.8536\n"
     ]
    }
   ],
   "source": [
    "best_xg = xgb_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb = best_xg.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Test Accuracy: {accuracy_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c3d56-49da-428e-bf7a-9095f521e83e",
   "metadata": {},
   "source": [
    "As we see here, the accuracy is 85.36% which is similar to the training accuracy as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
